{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAMAZAN ARSLAN - CMPE 492 PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec with NeuroBoun Corpus\n",
    "\n",
    "\n",
    "[Neuroboun](https://www.neuroboun.com), a section of the brain through [PUBMED](https://www.ncbi.nlm.nih.gov/pubmed/) about the amygdala about 37298 articles were taken.\n",
    "In Neuroboun, you can enter the keyword you want and see the rate of articles using the keyword.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Import\n",
    "\n",
    "First, we start with imports gensim (for word2vec), logging, django framework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "import logging\n",
    "import os,django\n",
    "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\",\"neuroboun.settings\")\n",
    "django.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Adding Logging Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Article Model\n",
    "We used Article model\n",
    "- abstract = models.TextField(null=True)\n",
    "- year = models.IntegerField()\n",
    "- pubmed_id = models.IntegerField(primary_key=True)\n",
    "- doi = models.TextField(null=True)\n",
    "- title = models.TextField()\n",
    "- keywords = models.TextField()\n",
    "\n",
    "importing the model and getting all object :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroextractor.models import Article\n",
    "articles=Article.objects.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Read articles into a list\n",
    "Now that we've had a sneak peak of our dataset, we can read it into a list so that we can pass this on to the Word2Vec model. \n",
    "\n",
    "Using `gensim.utils.simple_preprocess (line.title)`. \n",
    "\n",
    "This does some basic pre-processing such as **tokenization,** **lowercasing,** etc and returns back a list of tokens (words).\n",
    "\n",
    "\n",
    "\n",
    "- read the tokenized reviews into a list\n",
    "- each review item becomes a serries of words\n",
    "- so this becomes a list of lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-02 14:26:41,811 : INFO : reading NeuroBoun ...this may take a while\n",
      "2018-11-02 14:26:42,356 : INFO : read 0 reviews\n",
      "2018-11-02 14:26:42,383 : INFO : read 1000 reviews\n",
      "2018-11-02 14:26:42,411 : INFO : read 2000 reviews\n",
      "2018-11-02 14:26:42,440 : INFO : read 3000 reviews\n",
      "2018-11-02 14:26:42,466 : INFO : read 4000 reviews\n",
      "2018-11-02 14:26:42,491 : INFO : read 5000 reviews\n",
      "2018-11-02 14:26:42,523 : INFO : read 6000 reviews\n",
      "2018-11-02 14:26:42,548 : INFO : read 7000 reviews\n",
      "2018-11-02 14:26:42,575 : INFO : read 8000 reviews\n",
      "2018-11-02 14:26:42,604 : INFO : read 9000 reviews\n",
      "2018-11-02 14:26:42,633 : INFO : read 10000 reviews\n",
      "2018-11-02 14:26:42,662 : INFO : read 11000 reviews\n",
      "2018-11-02 14:26:42,690 : INFO : read 12000 reviews\n",
      "2018-11-02 14:26:42,719 : INFO : read 13000 reviews\n",
      "2018-11-02 14:26:42,755 : INFO : read 14000 reviews\n",
      "2018-11-02 14:26:42,856 : INFO : read 15000 reviews\n",
      "2018-11-02 14:26:42,883 : INFO : read 16000 reviews\n",
      "2018-11-02 14:26:42,912 : INFO : read 17000 reviews\n",
      "2018-11-02 14:26:42,941 : INFO : read 18000 reviews\n",
      "2018-11-02 14:26:42,969 : INFO : read 19000 reviews\n",
      "2018-11-02 14:26:42,997 : INFO : read 20000 reviews\n",
      "2018-11-02 14:26:43,026 : INFO : read 21000 reviews\n",
      "2018-11-02 14:26:43,057 : INFO : read 22000 reviews\n",
      "2018-11-02 14:26:43,089 : INFO : read 23000 reviews\n",
      "2018-11-02 14:26:43,114 : INFO : read 24000 reviews\n",
      "2018-11-02 14:26:43,141 : INFO : read 25000 reviews\n",
      "2018-11-02 14:26:43,167 : INFO : read 26000 reviews\n",
      "2018-11-02 14:26:43,194 : INFO : read 27000 reviews\n",
      "2018-11-02 14:26:43,223 : INFO : read 28000 reviews\n",
      "2018-11-02 14:26:43,252 : INFO : read 29000 reviews\n",
      "2018-11-02 14:26:43,281 : INFO : read 30000 reviews\n",
      "2018-11-02 14:26:43,312 : INFO : read 31000 reviews\n",
      "2018-11-02 14:26:43,339 : INFO : read 32000 reviews\n",
      "2018-11-02 14:26:43,369 : INFO : read 33000 reviews\n",
      "2018-11-02 14:26:43,395 : INFO : read 34000 reviews\n",
      "2018-11-02 14:26:43,427 : INFO : read 35000 reviews\n",
      "2018-11-02 14:26:43,454 : INFO : read 36000 reviews\n",
      "2018-11-02 14:26:43,485 : INFO : read 37000 reviews\n",
      "2018-11-02 14:26:43,508 : INFO : Done reading data file\n"
     ]
    }
   ],
   "source": [
    "def read_input(articles):\n",
    "       \n",
    "    logging.info(\"reading NeuroBoun ...this may take a while\")    \n",
    "    \n",
    "    for i, line in enumerate (articles): \n",
    "        if (i%1000==0):\n",
    "            logging.info (\"read {0} reviews\".format (i))\n",
    "\n",
    "        # do some pre-processing and return a list of words for each review text        \n",
    "        yield gensim.utils.simple_preprocess (line.title)\n",
    "        \n",
    "documents = list (read_input (articles))\n",
    "logging.info (\"Done reading data file\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training the Word2Vec model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Word2Vec model\n",
    "\n",
    "Training the model is fairly straightforward. You just instantiate Word2Vec and pass the reviews that we read in the previous step (the `documents`). So, we are essentially passing on a list of lists. Where each list within the main list contains a set of tokens from a user review. Word2Vec uses all these tokens to internally create a vocabulary. And by vocabulary, I mean a set of unique words.\n",
    "\n",
    "After building the vocabulary, we just need to call `train(...)` to start training the Word2Vec model. Training on the [OpinRank](http://kavita-ganesan.com/entity-ranking-data/) dataset takes about 10 minutes so please be patient while running your code on this dataset.\n",
    "\n",
    "Behind the scenes we are actually training a simple neural network with a single hidden layer. But, we are actually not going to use the neural network after training. Instead, the goal is to learn the weights of the hidden layer. These weights are essentially the word vectors that weâ€™re trying to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-02 14:26:50,350 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2018-11-02 14:26:50,351 : INFO : collecting all words and their counts\n",
      "2018-11-02 14:26:50,352 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-11-02 14:26:50,402 : INFO : PROGRESS: at sentence #10000, processed 139655 words, keeping 8950 word types\n",
      "2018-11-02 14:26:50,453 : INFO : PROGRESS: at sentence #20000, processed 279895 words, keeping 12488 word types\n",
      "2018-11-02 14:26:50,487 : INFO : PROGRESS: at sentence #30000, processed 419232 words, keeping 14864 word types\n",
      "2018-11-02 14:26:50,509 : INFO : collected 16148 word types from a corpus of 520241 raw words and 37298 sentences\n",
      "2018-11-02 14:26:50,510 : INFO : Loading a fresh vocabulary\n",
      "2018-11-02 14:26:50,537 : INFO : effective_min_count=2 retains 9572 unique words (59% of original 16148, drops 6576)\n",
      "2018-11-02 14:26:50,538 : INFO : effective_min_count=2 leaves 513665 word corpus (98% of original 520241, drops 6576)\n",
      "2018-11-02 14:26:50,567 : INFO : deleting the raw counts dictionary of 16148 items\n",
      "2018-11-02 14:26:50,569 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2018-11-02 14:26:50,570 : INFO : downsampling leaves estimated 382859 word corpus (74.5% of prior 513665)\n",
      "2018-11-02 14:26:50,600 : INFO : estimated required memory for 9572 words and 150 dimensions: 16272400 bytes\n",
      "2018-11-02 14:26:50,601 : INFO : resetting layer weights\n",
      "2018-11-02 14:26:50,720 : INFO : training model with 10 workers on 9572 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-11-02 14:26:50,996 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-02 14:26:51,004 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-02 14:26:51,016 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-02 14:26:51,033 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-02 14:26:51,035 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-02 14:26:51,036 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-02 14:26:51,037 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-02 14:26:51,048 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-02 14:26:51,057 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-02 14:26:51,063 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-02 14:26:51,065 : INFO : EPOCH - 1 : training on 520241 raw words (382889 effective words) took 0.3s, 1144028 effective words/s\n",
      "2018-11-02 14:26:51,391 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-02 14:26:51,392 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-02 14:26:51,418 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-02 14:26:51,420 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-02 14:26:51,421 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-02 14:26:51,421 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-02 14:26:51,428 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-02 14:26:51,431 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-02 14:26:51,438 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-02 14:26:51,450 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-02 14:26:51,451 : INFO : EPOCH - 2 : training on 520241 raw words (382754 effective words) took 0.4s, 1040827 effective words/s\n",
      "2018-11-02 14:26:51,867 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-02 14:26:51,910 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-02 14:26:51,913 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-02 14:26:51,914 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-02 14:26:51,915 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-02 14:26:51,935 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-02 14:26:51,939 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-02 14:26:51,951 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-02 14:26:51,955 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-02 14:26:51,959 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-02 14:26:51,960 : INFO : EPOCH - 3 : training on 520241 raw words (382714 effective words) took 0.5s, 788114 effective words/s\n",
      "2018-11-02 14:26:52,341 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-02 14:26:52,342 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-02 14:26:52,352 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-02 14:26:52,361 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-02 14:26:52,364 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-02 14:26:52,366 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-02 14:26:52,391 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-02 14:26:52,398 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-02 14:26:52,400 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-02 14:26:52,412 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-02 14:26:52,413 : INFO : EPOCH - 4 : training on 520241 raw words (382616 effective words) took 0.4s, 876007 effective words/s\n",
      "2018-11-02 14:26:52,737 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-02 14:26:52,749 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-02 14:26:52,750 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-02 14:26:52,753 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-02 14:26:52,772 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-02 14:26:52,776 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-02 14:26:52,786 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-02 14:26:52,792 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-02 14:26:52,795 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-02 14:26:52,799 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-02 14:26:52,800 : INFO : EPOCH - 5 : training on 520241 raw words (382915 effective words) took 0.4s, 1030506 effective words/s\n",
      "2018-11-02 14:26:52,800 : INFO : training on a 2601205 raw words (1913888 effective words) took 2.1s, 920412 effective words/s\n",
      "2018-11-02 14:26:52,801 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2018-11-02 14:26:52,802 : INFO : training model with 10 workers on 9572 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-11-02 14:26:53,118 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-02 14:26:53,121 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-02 14:26:53,123 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-02 14:26:53,126 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-02 14:26:53,127 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-02 14:26:53,138 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-02 14:26:53,140 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-02 14:26:53,140 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-02 14:26:53,151 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-02 14:26:53,158 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-02 14:26:53,159 : INFO : EPOCH - 1 : training on 520241 raw words (382751 effective words) took 0.3s, 1136824 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-02 14:26:53,473 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-02 14:26:53,487 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-02 14:26:53,523 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-02 14:26:53,525 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-02 14:26:53,532 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-02 14:26:53,536 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-02 14:26:53,539 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-02 14:26:53,543 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-02 14:26:53,551 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-02 14:26:53,558 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-02 14:26:53,559 : INFO : EPOCH - 2 : training on 520241 raw words (383093 effective words) took 0.4s, 993074 effective words/s\n",
      "2018-11-02 14:26:53,876 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-02 14:26:53,877 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-02 14:26:53,901 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-02 14:26:53,905 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-02 14:26:53,908 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-02 14:26:53,910 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-02 14:26:53,916 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-02 14:26:53,917 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-02 14:26:53,923 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-02 14:26:53,924 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-02 14:26:53,925 : INFO : EPOCH - 3 : training on 520241 raw words (382690 effective words) took 0.4s, 1084468 effective words/s\n",
      "2018-11-02 14:26:54,247 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-02 14:26:54,252 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-02 14:26:54,256 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-02 14:26:54,257 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-02 14:26:54,258 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-02 14:26:54,259 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-02 14:26:54,267 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-02 14:26:54,273 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-02 14:26:54,276 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-02 14:26:54,284 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-02 14:26:54,284 : INFO : EPOCH - 4 : training on 520241 raw words (382788 effective words) took 0.3s, 1098493 effective words/s\n",
      "2018-11-02 14:26:54,620 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-02 14:26:54,623 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-02 14:26:54,629 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-02 14:26:54,648 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-02 14:26:54,651 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-02 14:26:54,653 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-02 14:26:54,668 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-02 14:26:54,672 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-02 14:26:54,674 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-02 14:26:54,680 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-02 14:26:54,681 : INFO : EPOCH - 5 : training on 520241 raw words (382810 effective words) took 0.4s, 994608 effective words/s\n",
      "2018-11-02 14:26:55,062 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-02 14:26:55,065 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-02 14:26:55,066 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-02 14:26:55,068 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-02 14:26:55,068 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-02 14:26:55,069 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-02 14:26:55,079 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-02 14:26:55,089 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-02 14:26:55,092 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-02 14:26:55,100 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-02 14:26:55,102 : INFO : EPOCH - 6 : training on 520241 raw words (382769 effective words) took 0.4s, 948656 effective words/s\n",
      "2018-11-02 14:26:55,376 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-02 14:26:55,384 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-02 14:26:55,396 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-02 14:26:55,404 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-02 14:26:55,410 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-02 14:26:55,417 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-02 14:26:55,423 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-02 14:26:55,425 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-02 14:26:55,426 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-02 14:26:55,430 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-02 14:26:55,430 : INFO : EPOCH - 7 : training on 520241 raw words (382880 effective words) took 0.3s, 1225935 effective words/s\n",
      "2018-11-02 14:26:55,740 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-02 14:26:55,742 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-02 14:26:55,743 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-02 14:26:55,744 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-02 14:26:55,756 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-02 14:26:55,767 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-02 14:26:55,768 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-02 14:26:55,772 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-02 14:26:55,775 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-02 14:26:55,785 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-02 14:26:55,786 : INFO : EPOCH - 8 : training on 520241 raw words (382969 effective words) took 0.3s, 1112923 effective words/s\n",
      "2018-11-02 14:26:56,103 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-02 14:26:56,104 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-02 14:26:56,118 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-02 14:26:56,119 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-02 14:26:56,137 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-02 14:26:56,146 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-02 14:26:56,159 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-02 14:26:56,170 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-02 14:26:56,171 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-02 14:26:56,173 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-02 14:26:56,174 : INFO : EPOCH - 9 : training on 520241 raw words (382976 effective words) took 0.4s, 1025205 effective words/s\n",
      "2018-11-02 14:26:56,503 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-11-02 14:26:56,505 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-11-02 14:26:56,506 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-11-02 14:26:56,522 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-02 14:26:56,523 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-02 14:26:56,531 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-02 14:26:56,539 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-02 14:26:56,543 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-02 14:26:56,546 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-02 14:26:56,550 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-02 14:26:56,551 : INFO : EPOCH - 10 : training on 520241 raw words (382758 effective words) took 0.4s, 1061624 effective words/s\n",
      "2018-11-02 14:26:56,552 : INFO : training on a 5202410 raw words (3828484 effective words) took 3.7s, 1021069 effective words/s\n",
      "2018-11-02 14:26:56,553 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('amygdalar', 0.6904447078704834),\n",
       " ('amygdaloid', 0.6506998538970947),\n",
       " ('dorsal', 0.5091315507888794),\n",
       " ('septum', 0.4866825044155121),\n",
       " ('ventral', 0.45731252431869507),\n",
       " ('bla', 0.45720943808555603),\n",
       " ('geniculate', 0.4503151476383209),\n",
       " ('habenular', 0.44185322523117065),\n",
       " ('accumbens', 0.4305647313594818),\n",
       " ('amygdale', 0.4303969144821167)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec (documents, size=150, window=5, min_count=2, workers=10)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)\n",
    "\n",
    "w1 = \"amygdala\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's look at some output \n",
    "This first example shows a simple case of looking up words similar to the word `amygdala`. All we need to do here is to call the `most_similar` function and provide the word `amygdala` as the positive example. This returns the top 10 similar words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amygdalar', 0.6844335794448853),\n",
       " ('amygdaloid', 0.6229093074798584),\n",
       " ('ventral', 0.5053435564041138),\n",
       " ('septum', 0.4974673390388489),\n",
       " ('bla', 0.4948784112930298),\n",
       " ('geniculate', 0.48149192333221436),\n",
       " ('dorsal', 0.480157732963562),\n",
       " ('amygdale', 0.4364165663719177),\n",
       " ('prelimbic', 0.4177556037902832),\n",
       " ('habenula', 0.4051814079284668)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w1 = \"amygdala\"\n",
    "model.wv.most_similar (positive=w1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65069985"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two different words\n",
    "model.wv.similarity(w1=\"amygdala\",w2=\"amygdaloid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10858928"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two unrelated words\n",
    "model.wv.similarity(w1=\"amygdala\",w2=\"brain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the odd one out\n",
    "You can even use Word2Vec to find odd items given a list of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'animal'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"male\",\"female\",\"animal\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding some of the parameters\n",
    "To train the model earlier, we had to set some parameters. Now, let's try to understand what some of them mean. For reference, this is the command that we used to train the model.\n",
    "\n",
    "```\n",
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)\n",
    "```\n",
    "\n",
    "### `size`\n",
    "The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. A value of 100-150 has worked well for me. \n",
    "\n",
    "### `window`\n",
    "The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too much, as long as its a decent sized window. \n",
    "\n",
    "### `min_count`\n",
    "Minimium frequency count of words. The model would ignore words that do not statisfy the `min_count`. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n",
    "\n",
    "### `workers`\n",
    "How many threads to use behind the scenes?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
